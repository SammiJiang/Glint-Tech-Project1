{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391ace3e-149a-444f-a823-63f59258f8ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Project 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237239ab-ecb0-4a9b-be11-2488cb0dec72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\n+--------------------+-------------+------------+-------------------+--------------+---------------+--------------------+------------+--------------------+-----+--------------------+\\n|         business_id|         city|current_flag|               date|effective_date|expiration_date|                name|review_count|           review_id|state|                text|\\n+--------------------+-------------+------------+-------------------+--------------+---------------+--------------------+------------+--------------------+-----+--------------------+\\n|GILL0ZkvVXJaNQzYR...|  New Orleans|        true|2012-10-08 19:32:44|    2023-10-12|     9999-12-31|Paris Parker Salo...|         119|cejwyz1Hn1yQ-v2m9...|   LA|I absolutely love...|\\n|C7ZNfxgDQWinNzRfT...| Philadelphia|        true|2014-03-14 22:11:21|    2023-10-12|     9999-12-31|The Men's Club Ba...|          62|1sZkJCitG9-XSlZQJ...|   PA|Men's clubs is th...|\\n|bMVgakLiCEA6LWRUE...|         Reno|        true|2015-05-12 17:42:56|    2023-10-12|     9999-12-31|  Express Smog Check|          37|SFy1mkQaiYFrWqoeB...|   NV|I love coming her...|\\n|dIpTuL2T1L98GQGU1...|        Tampa|        true|2012-03-02 03:16:56|    2023-10-12|     9999-12-31|          The Bricks|         548|dFgGps6gQQbWoKOR6...|   FL|My friend and I v...|\\n|WJ17PIWEaqrNZ07un...|    Sahuarita|        true|2018-07-17 08:37:06|    2023-10-12|     9999-12-31|Green Valley Mort...|           5|NU4_03KfndoUcE8mD...|   AZ|The only reason I...|\\n|USekrAG0-4tJUs9V2...|      Dunedin|        true|2016-10-14 01:12:42|    2023-10-12|     9999-12-31|            The Honu|         234|lkdzFSsG-W1v7pP3n...|   FL|Stopped in tonigh...|\\n|IDtLPgUrqorrpqSLd...|Santa Barbara|        true|2018-07-17 00:20:41|    2023-10-12|     9999-12-31|Helena Avenue Bakery|         389|Q3fPo_x6xKxafAzy1...|   CA|Pricey ( a ham an...|\\n|MHqeqoJEjsTHivmuX...|  Saint Louis|        true|2013-12-29 01:22:34|    2023-10-12|     9999-12-31|       3500 Winehaus|          40|IrQyuZfiMKRkh3V-0...|   MO|Really great plac...|\\n|9uHEhLKTVXATCt7JS...| Philadelphia|        true|2010-07-07 16:58:43|    2023-10-12|     9999-12-31|    John's Water Ice|         274|biClS4h8ZNYEVrlc7...|   PA|Calories and diab...|\\n|tP8wd-9CrI_RkHK42...|       Tucson|        true|2013-01-27 23:05:05|    2023-10-12|     9999-12-31|        Panera Bread|         128|iFRIf-_U8cIW0rC5t...|   AZ|They call a chick...|\\n|OHzX-ZD9qyoeoxR8Z...| Philadelphia|        true|2014-07-28 00:27:44|    2023-10-12|     9999-12-31|     Khyber Pass Pub|         845|1V0bzX70ZuP7KJXIJ...|   PA|Had a wonderful l...|\\n|sr-5EY6bmp4jINhea...| Indianapolis|        true|2017-04-08 01:47:10|    2023-10-12|     9999-12-31|The Cake Bake Sho...|         996|8bGJJrY9zhMpKqIkm...|   IN|The experience is...|\\n|_RwlMTw9uFeOkfX9C...| Philadelphia|        true|2014-01-17 07:22:52|    2023-10-12|     9999-12-31|Dinardo's Famous ...|         256|rihdCbit6O5sGOV9I...|   PA|Thankfully I had ...|\\n|ADgeB1sfOGbzCR3LI...|  New Orleans|        true|2014-02-05 22:30:59|    2023-10-12|     9999-12-31|      Cafe Reconcile|         152|slE0DDjPefSStdL4D...|   LA|Went to lunch tod...|\\n|encfXG_jrG1M6gX1I...|        Tampa|        true|2016-11-10 14:28:28|    2023-10-12|     9999-12-31|      Ballyhoo Grill|         262|_nB4sSn0ytRkNEOol...|   FL|Pretty good house...|\\n|50MNOCMK3OxqR_VL3...| Philadelphia|        true|2013-10-24 15:44:32|    2023-10-12|     9999-12-31|        Sprint Store|          15|q3MLkJC3KI1IO-t6G...|   PA|I went to this sp...|\\n|mUIBtlWNPD7sz3rGG...| Philadelphia|        true|2008-12-05 14:16:05|    2023-10-12|     9999-12-31|             Kanella|         579|-nwOShwCyytQDHrq9...|   PA|Great for lunch! ...|\\n|vOgQnvKbE4nMopFTj...|    Nashville|        true|2011-02-13 19:46:20|    2023-10-12|     9999-12-31|Watermark Restaurant|         296|lonLaQxyruKIVuxQ6...|   TN|MY favorite place...|\\n|hEMNTKjcnhHxw9NRp...|   Norristown|        true|2014-12-17 18:20:29|    2023-10-12|     9999-12-31|       Beagle Tavern|          15|kwUvMHrp0ZZ1PqLS-...|   PA|My days as the du...|\\n|0pL-eq0ufX5jMDgVA...| Sicklerville|        true|2017-05-13 23:29:39|    2023-10-12|     9999-12-31|              Sakura|          73|Xe__9plcT4VI1oAfv...|   NJ|2nd time here on ...|\\n+--------------------+-------------+------------+-------------------+--------------+---------------+--------------------+------------+--------------------+-----+--------------------+\\nonly showing top 20 rows\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Project 1 \n",
    "@Author: Sammi Jiang \n",
    "This project demonstrated an SCD type II procedure from raw data -> bronze ->silver, and a couple of analyzing cases are provided \n",
    "- For demo purpose, only two files are read and processed here \n",
    "    - yelp_academic_dataset_business\n",
    "    - yelp_academic_dataset_review\n",
    "\"\"\"\n",
    "\n",
    "# Import session \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os   # Add import for os to use os.path.exists()\n",
    "raw_biz_path = \"dbfs:/FileStore/tables/yelp/yelp_academic_dataset_business.json\"\n",
    "raw_review_path = \"dbfs:/FileStore/tables/yelp/yelp_academic_dataset_review_*.json\"\n",
    "DATE_FORMAT = \"yyyy-MM-dd\"\n",
    "EOW_DATE = \"9999-12-31\"\n",
    "SOURCE_PATH = \"FileStore/tables/yelp/current_data.json\"\n",
    "DEST_PATH = \"FileStore/tables/SCD/archive.json\"\n",
    "\n",
    "def read_data(biz, review):\n",
    "    \"\"\"\n",
    "    Processes business and review data from JSON files and returns them as dataframes.\n",
    "\n",
    "    @param biz: str\n",
    "        Path to the business JSON file containing information like address, ID, categories, etc.\n",
    "    \n",
    "    @param review: str\n",
    "        Path to the review JSON file containing review data.\n",
    "\n",
    "    @return: tuple\n",
    "        A tuple of processed business DataFrame and unprocessed review DataFrame.\n",
    "\n",
    "    @description:\n",
    "        Reads and processes the business JSON file, flattens attributes, and explodes categories into separate rows.\n",
    "        Reads the review JSON file without processing.\n",
    "    \"\"\"\n",
    "    df_biz = spark.read.option(\"header\", \"true\").json(biz)\n",
    "    attr_cols = [col(\"attributes.\" + attr).alias(attr) for attr in df_biz.select(\"attributes.*\").columns]\n",
    "\n",
    "# Select other columns and attributes columns together\n",
    "    df_biz = df_biz.select(\"address\", \"business_id\", \"categories\", \"city\", \"hours\", \"is_open\", \n",
    "                            \"latitude\", \"longitude\", \"name\", \"postal_code\", \"review_count\", \n",
    "                            \"stars\", \"state\", *attr_cols)\n",
    "\n",
    "# Split the categories string into an array\n",
    "    df_biz = df_biz.withColumn(\"categories\", split(col(\"categories\"), \", \"))\n",
    "\n",
    "# Explode the categories array into multiple rows\n",
    "    df_biz = df_biz.withColumn(\"category\", explode(col(\"categories\")))\n",
    "\n",
    "# Drop the original categories column if needed\n",
    "    df_biz = df_biz.drop(\"categories\")\n",
    "    df_review = spark.read.option(\"header\", \"true\").json(review)\n",
    "\n",
    "    return df_biz, df_review\n",
    "\n",
    "def create_current_raw(df_biz, df_review):\n",
    "    \"\"\"\n",
    "    Joins and cleans business and review data.\n",
    "\n",
    "    @param df_biz: DataFrame\n",
    "        Processed business data.\n",
    "    \n",
    "    @param df_review: DataFrame\n",
    "        Unprocessed review data.\n",
    "\n",
    "    @return: DataFrame\n",
    "        Cleaned, joined data with selected columns, distinct entries, and no null values.\n",
    "\n",
    "    @description:\n",
    "        Joins business and review data on business ID, selects relevant columns, and removes duplicate and null entries.\n",
    "    \"\"\"\n",
    "    view = df_review.join(df_biz, df_review['business_id'] == df_biz['business_id'], 'right')\n",
    "    view = view.select(\n",
    "        df_biz[\"business_id\"],\n",
    "        view['review_count'],\n",
    "        view[\"review_id\"],\n",
    "        view[\"text\"],\n",
    "        view['date'],\n",
    "        view['state'],\n",
    "        view['city'],\n",
    "        view['name'],\n",
    "        view['category']\n",
    "    ).distinct().dropna()\n",
    "\n",
    "    return view\n",
    "\n",
    "def raw_to_bronze(view):\n",
    "    \"\"\"\n",
    "    Transforms raw data to a bronze-level dataset with added metadata and limited sample size.\n",
    "\n",
    "    @param view: DataFrame\n",
    "        The raw, joined data of business and review information.\n",
    "\n",
    "    @return: DataFrame\n",
    "        A bronze-level data enriched with effective and expiration dates, and a current flag, limited to 10,000 rows.\n",
    "\n",
    "    @description:\n",
    "        Enriches the raw data with metadata including the effective date, expiration date, and a flag indicating the \n",
    "        current data. For demo purposes, the dataset is limited to 10,000 rows to enhance processing speed.\n",
    "    \"\"\"\n",
    "    df_current = view \\\n",
    "        .withColumn(\"effective_date\", date_format(current_date(), DATE_FORMAT)) \\\n",
    "        .withColumn(\"expiration_date\", date_format(lit(EOW_DATE), DATE_FORMAT)) \\\n",
    "        .withColumn(\"current_flag\", lit(True)) \\\n",
    "        .limit(10000)\n",
    "\n",
    "    return df_current\n",
    "\n",
    "def bronze_to_silver():\n",
    "    \"\"\"\n",
    "    Transforms the bronze data to silver by handling updates and inserts and combining them with historical data.\n",
    "\n",
    "    @return: DataFrame\n",
    "        A silver-level dataset updated with the current data and enriched with historical records.\n",
    "\n",
    "    @description:\n",
    "        Identifies updated and inserted records from the current data, updates historical records accordingly, and\n",
    "        combines them to create a comprehensive, up-to-date silver-level dataset.\n",
    "    \"\"\"\n",
    "    df_current = spark.read.option(\"header\", \"true\").json(\"dbfs:/FileStore/tables/yelp/current_data.json\")\n",
    "    historical_data = spark.read.option(\"header\", \"true\").json(\"dbfs:/FileStore/tables/SCD/archive.json\")\n",
    "        \n",
    "\n",
    "    columns_to_compare = [\"name\"]\n",
    "\n",
    "    # Create condition for identifying updates\n",
    "    condition = ' OR '.join(\n",
    "        [f'(a.{col} != b.{col} OR (a.{col} IS NULL AND b.{col} IS NOT NULL) OR (a.{col} IS NOT NULL AND b.{col} IS NULL))'\n",
    "        for col in columns_to_compare]\n",
    "    )\n",
    "\n",
    "    # Identify updated and inserted records\n",
    "    updates = df_current.alias(\"a\").join(historical_data.alias(\"b\"), \"business_id\", how=\"inner\").filter(F.expr(condition))\n",
    "    inserts = df_current.alias(\"a\").join(historical_data.alias(\"b\"), \"business_id\", how=\"left_anti\")\n",
    "\n",
    "    # Collect updated keys - Optimized to avoid RDD\n",
    "    updated_keys = [row.business_id for row in updates.select(\"business_id\").distinct().collect()]\n",
    "\n",
    "    # Handle updates in the historical data\n",
    "    historical_data = historical_data.withColumn(\n",
    "        \"current_flag\", \n",
    "        F.when(F.col(\"business_id\").isin(updated_keys), F.lit(False)).otherwise(F.col(\"current_flag\"))\n",
    "    ).withColumn(\n",
    "        \"expiration_date\", \n",
    "        F.when(F.col(\"business_id\").isin(updated_keys), F.date_sub(F.current_date(), 1)).otherwise(F.col(\"expiration_date\"))\n",
    "    )\n",
    "\n",
    "    # Prepare updated records - Selecting columns explicitly to avoid ambiguity\n",
    "    updates = updates.select([F.col(f'a.{col}').alias(col) for col in df_current.columns]) \\\n",
    "        .withColumn(\"effective_date\", F.current_date()) \\\n",
    "        .withColumn(\"expiration_date\", F.lit(EOW_DATE)) \\\n",
    "        .withColumn(\"current_flag\", F.lit(True))\n",
    "\n",
    "    # Prepare new records\n",
    "    inserts = inserts.withColumn(\"effective_date\", F.current_date()) \\\n",
    "        .withColumn(\"expiration_date\", F.lit(EOW_DATE)) \\\n",
    "        .withColumn(\"current_flag\", F.lit(True))\n",
    "\n",
    "    # Union updated and new records with historical data\n",
    "    final_df = historical_data.union(updates.select(*historical_data.columns)).union(inserts.select(*historical_data.columns))\n",
    "\n",
    "\n",
    "    return final_df\n",
    "\n",
    "    # ... (previous parts of the script)\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"test\") \\\n",
    "        .config(\"test\", \"somevalue\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "    df_biz, df_review = read_data(raw_biz_path, raw_review_path)\n",
    "    view = create_current_raw(df_biz, df_review)\n",
    "    df_current = raw_to_bronze(view)\n",
    "\n",
    "    # Write df_current to the source path\n",
    "    df_current.write \\\n",
    "        .format(\"json\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(SOURCE_PATH)\n",
    "\n",
    "    if not os.path.exists(DEST_PATH):\n",
    "        df_current.write \\\n",
    "            .format(\"json\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(DEST_PATH)\n",
    "\n",
    "    df_final = bronze_to_silver()\n",
    "    df_final.write \\\n",
    "        .format(\"json\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(DEST_PATH)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\"\"\"\n",
    "+--------------------+-------------+------------+-------------------+--------------+---------------+--------------------+------------+--------------------+-----+--------------------+\n",
    "|         business_id|         city|current_flag|               date|effective_date|expiration_date|                name|review_count|           review_id|state|                text|\n",
    "+--------------------+-------------+------------+-------------------+--------------+---------------+--------------------+------------+--------------------+-----+--------------------+\n",
    "|GILL0ZkvVXJaNQzYR...|  New Orleans|        true|2012-10-08 19:32:44|    2023-10-12|     9999-12-31|Paris Parker Salo...|         119|cejwyz1Hn1yQ-v2m9...|   LA|I absolutely love...|\n",
    "|C7ZNfxgDQWinNzRfT...| Philadelphia|        true|2014-03-14 22:11:21|    2023-10-12|     9999-12-31|The Men's Club Ba...|          62|1sZkJCitG9-XSlZQJ...|   PA|Men's clubs is th...|\n",
    "|bMVgakLiCEA6LWRUE...|         Reno|        true|2015-05-12 17:42:56|    2023-10-12|     9999-12-31|  Express Smog Check|          37|SFy1mkQaiYFrWqoeB...|   NV|I love coming her...|\n",
    "|dIpTuL2T1L98GQGU1...|        Tampa|        true|2012-03-02 03:16:56|    2023-10-12|     9999-12-31|          The Bricks|         548|dFgGps6gQQbWoKOR6...|   FL|My friend and I v...|\n",
    "|WJ17PIWEaqrNZ07un...|    Sahuarita|        true|2018-07-17 08:37:06|    2023-10-12|     9999-12-31|Green Valley Mort...|           5|NU4_03KfndoUcE8mD...|   AZ|The only reason I...|\n",
    "|USekrAG0-4tJUs9V2...|      Dunedin|        true|2016-10-14 01:12:42|    2023-10-12|     9999-12-31|            The Honu|         234|lkdzFSsG-W1v7pP3n...|   FL|Stopped in tonigh...|\n",
    "|IDtLPgUrqorrpqSLd...|Santa Barbara|        true|2018-07-17 00:20:41|    2023-10-12|     9999-12-31|Helena Avenue Bakery|         389|Q3fPo_x6xKxafAzy1...|   CA|Pricey ( a ham an...|\n",
    "|MHqeqoJEjsTHivmuX...|  Saint Louis|        true|2013-12-29 01:22:34|    2023-10-12|     9999-12-31|       3500 Winehaus|          40|IrQyuZfiMKRkh3V-0...|   MO|Really great plac...|\n",
    "|9uHEhLKTVXATCt7JS...| Philadelphia|        true|2010-07-07 16:58:43|    2023-10-12|     9999-12-31|    John's Water Ice|         274|biClS4h8ZNYEVrlc7...|   PA|Calories and diab...|\n",
    "|tP8wd-9CrI_RkHK42...|       Tucson|        true|2013-01-27 23:05:05|    2023-10-12|     9999-12-31|        Panera Bread|         128|iFRIf-_U8cIW0rC5t...|   AZ|They call a chick...|\n",
    "|OHzX-ZD9qyoeoxR8Z...| Philadelphia|        true|2014-07-28 00:27:44|    2023-10-12|     9999-12-31|     Khyber Pass Pub|         845|1V0bzX70ZuP7KJXIJ...|   PA|Had a wonderful l...|\n",
    "|sr-5EY6bmp4jINhea...| Indianapolis|        true|2017-04-08 01:47:10|    2023-10-12|     9999-12-31|The Cake Bake Sho...|         996|8bGJJrY9zhMpKqIkm...|   IN|The experience is...|\n",
    "|_RwlMTw9uFeOkfX9C...| Philadelphia|        true|2014-01-17 07:22:52|    2023-10-12|     9999-12-31|Dinardo's Famous ...|         256|rihdCbit6O5sGOV9I...|   PA|Thankfully I had ...|\n",
    "|ADgeB1sfOGbzCR3LI...|  New Orleans|        true|2014-02-05 22:30:59|    2023-10-12|     9999-12-31|      Cafe Reconcile|         152|slE0DDjPefSStdL4D...|   LA|Went to lunch tod...|\n",
    "|encfXG_jrG1M6gX1I...|        Tampa|        true|2016-11-10 14:28:28|    2023-10-12|     9999-12-31|      Ballyhoo Grill|         262|_nB4sSn0ytRkNEOol...|   FL|Pretty good house...|\n",
    "|50MNOCMK3OxqR_VL3...| Philadelphia|        true|2013-10-24 15:44:32|    2023-10-12|     9999-12-31|        Sprint Store|          15|q3MLkJC3KI1IO-t6G...|   PA|I went to this sp...|\n",
    "|mUIBtlWNPD7sz3rGG...| Philadelphia|        true|2008-12-05 14:16:05|    2023-10-12|     9999-12-31|             Kanella|         579|-nwOShwCyytQDHrq9...|   PA|Great for lunch! ...|\n",
    "|vOgQnvKbE4nMopFTj...|    Nashville|        true|2011-02-13 19:46:20|    2023-10-12|     9999-12-31|Watermark Restaurant|         296|lonLaQxyruKIVuxQ6...|   TN|MY favorite place...|\n",
    "|hEMNTKjcnhHxw9NRp...|   Norristown|        true|2014-12-17 18:20:29|    2023-10-12|     9999-12-31|       Beagle Tavern|          15|kwUvMHrp0ZZ1PqLS-...|   PA|My days as the du...|\n",
    "|0pL-eq0ufX5jMDgVA...| Sicklerville|        true|2017-05-13 23:29:39|    2023-10-12|     9999-12-31|              Sakura|          73|Xe__9plcT4VI1oAfv...|   NJ|2nd time here on ...|\n",
    "+--------------------+-------------+------------+-------------------+--------------+---------------+--------------------+------------+--------------------+-----+--------------------+\n",
    "only showing top 20 rows\n",
    "\"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
